{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Met2 Dataset\n",
    "\n",
    "japanese encoding : https://en.wikipedia.org/wiki/JIS_encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip \n",
    "\n",
    "met2_path = '/media/D/ADL2020-SPRING/project/met2/met2data/training/japanese/keys/keys.970827.gz'\n",
    "\n",
    "with gzip.open(met2_path, 'rb') as f:\n",
    "    file_content = f.read()\n",
    "document = str(file_content, 'EUC-JP')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def get_samples(document):\n",
    "    b_sentence, b_label = [], []\n",
    "\n",
    "    soup = BeautifulSoup(document, 'html.parser')\n",
    "    doc_list = list(soup.find_all('doc'))\n",
    "\n",
    "    for doc in doc_list:\n",
    "        paragraphs = list(doc.find_all('p'))\n",
    "        for paragraph in paragraphs:\n",
    "            p = paragraph\n",
    "\n",
    "            # labeling\n",
    "            cursor, sentence, label = 0, [], []\n",
    "\n",
    "            tag_list = list(p.find_all(['enamex','timex','numex']))\n",
    "            for tag in tag_list:\n",
    "                cursor_tag = p.text.find(tag.text, cursor, len(p.text))\n",
    "\n",
    "                if cursor_tag-cursor>0:\n",
    "                    sentence.append(p.text[cursor:cursor_tag].replace('\\u3000','').replace('\\n',''))\n",
    "                    label.append('None')\n",
    "                sentence.append(p.text[cursor_tag:cursor_tag+len(tag.text)])\n",
    "                label.append(tag['type'])\n",
    "\n",
    "                cursor = cursor_tag+len(tag.text)\n",
    "\n",
    "            ## paragraph太長了，sentence超過5 當作下一筆sample\n",
    "            for i in range(0,len(sentence)-5,5):\n",
    "                b_sentence.append(sentence[i:i+5])\n",
    "                b_label.append(label[i:i+5])\n",
    "\n",
    "    return b_sentence, b_label\n",
    "\n",
    "met2_path = '/media/D/ADL2020-SPRING/project/met2/met2data/training/japanese/keys/keys.970827.gz'\n",
    "\n",
    "with gzip.open(met2_path, 'rb') as f:\n",
    "    file_content = f.read()\n",
    "document = str(file_content, 'EUC-JP')\n",
    "\n",
    "b_sentence, b_label = get_samples(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(860, 860)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(b_sentence), len(b_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['カルロス',\n",
       "  'さんは「',\n",
       "  'クリスマス',\n",
       "  'の奇跡だ」と言って絶句したという。現場は機体の破片が長さ１５０メートルにわたって散乱。軍当局が一度は「生存者なし」と確認した後に、次々と生存者が見つかったことで、救助関係者の間に驚きが広がった。',\n",
       "  'レイエス'],\n",
       " ['PERSON', 'None', 'DATE', 'None', 'PERSON'])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = 859\n",
    "b_sentence[idx], b_label[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['None',\n",
       " 'DATE',\n",
       " 'LOCATION',\n",
       " 'MONEY',\n",
       " 'ORGANIZATION',\n",
       " 'PERCENT',\n",
       " 'PERSON',\n",
       " 'TIME']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def get_tags(document):\n",
    "    tags = set()\n",
    "    soup = BeautifulSoup(document, 'html.parser')\n",
    "    name_tags = soup.find_all(['enamex','timex','numex'])\n",
    "    for tag in name_tags:\n",
    "        tags.update([tag['type']])\n",
    "    return ['None']+sorted(list(tags))\n",
    "\n",
    "met2_path = '/media/D/ADL2020-SPRING/project/met2/met2data/training/japanese/keys/keys.970827.gz'\n",
    "\n",
    "with gzip.open(met2_path, 'rb') as f:\n",
    "    document = str(f.read(), 'EUC-JP')\n",
    "    \n",
    "tags = get_tags(document)\n",
    "tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'random' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-2781e7f5475c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mdoc_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'doc'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'random' is not defined"
     ]
    }
   ],
   "source": [
    "met2_path = '/media/D/ADL2020-SPRING/project/met2/met2data/training/japanese/keys/keys.970827.gz'\n",
    "\n",
    "with gzip.open(met2_path, 'rb') as f:\n",
    "    document = str(f.read(), 'EUC-JP')\n",
    "    \n",
    "soup = BeautifulSoup(document, 'html.parser')\n",
    "\n",
    "doc_list = list(soup.find_all('doc'))\n",
    "#random.shuffle(doc_list)\n",
    "\n",
    "for doc in doc_list:\n",
    "    para = doc.find_all('p')\n",
    "    #print(len(para),para)\n",
    "    input(\"\")\n",
    "    for p in para:\n",
    "        print(p.text)\n",
    "        data.append(p.text)\n",
    "        input(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "para = para.find_next('enamex')\n",
    "para['type'], para.text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "para = para.next_sibling\n",
    "para"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cinnamon Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "########################################################\n",
    "##################  Cinnamon Dataset  ##################\n",
    "class Cinnamon_Dataset(Dataset):\n",
    "    def __init__(self, cinnamon_path, tokenizer):\n",
    "        def get_tags(cinnamon_path):\n",
    "            tags = set()\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "                label_str = filter(lambda i:(type(i) is str), dataframe['Tag'])\n",
    "                def split(strings):\n",
    "                    out = list()\n",
    "                    for string in strings: \n",
    "                        out += string.split(\";\")\n",
    "                    return out\n",
    "                items = split(label_str)\n",
    "                tags.update(items)\n",
    "            return tuple(sorted(list(tags)))\n",
    "            #return tuple([\"[PAD]\", \"[None]\"] + sorted(list(tags)))\n",
    "        \n",
    "        def get_samples(cinnamon_path):\n",
    "            groups = []\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "\n",
    "                p_index = dataframe.groupby('Parent Index')\n",
    "                for g in list(p_index.groups.keys()):\n",
    "                    groups.append(p_index.get_group(g))\n",
    "            return groups\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = get_samples(cinnamon_path)\n",
    "        self.tags = get_tags(cinnamon_path)\n",
    "\n",
    "        print(f'\\t[Info] Load Cannon_Dataset complete !! len:{self.__len__()}')    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "            \n",
    "    def collate_fn(self, samples):        \n",
    "        tokenizer, tags = self.tokenizer, self.tags\n",
    "            \n",
    "        CLS, SEP, PAD = tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id\n",
    "        \n",
    "        def zero_vec(): \n",
    "            return [0]*len(tags)\n",
    "        \n",
    "        ## text tokenized, label vectoized\n",
    "        b_token_ids, b_output = [], []\n",
    "        for sample in samples:\n",
    "            token_ids = [CLS]\n",
    "            output = [zero_vec()]\n",
    "            for text, tag in zip(sample['Text'],sample['Tag']):\n",
    "                ids = tokenizer.encode(text)[1:-1] + [SEP]\n",
    "                label = zero_vec()\n",
    "                if isinstance(tag, str): \n",
    "                    for t in tag.split(';'):\n",
    "                        label[tags.index(t)] = 1\n",
    "                token_ids += ids\n",
    "                output += [label]*(len(ids)-1) + [zero_vec()]\n",
    "            b_token_ids.append(token_ids)\n",
    "            b_output.append(output)\n",
    "\n",
    "        ## pad to same lenght\n",
    "        max_len = min(max([len(s) for s in b_token_ids]), 512)\n",
    "        for token_ids, output in zip(b_token_ids, b_output):\n",
    "            token_ids += [PAD]*(max_len-len(token_ids))\n",
    "            output += [zero_vec()]*(max_len-len(output))\n",
    "\n",
    "        return torch.tensor(b_token_ids), torch.tensor(b_output)\n",
    "        \n",
    "        \n",
    "from transformers import BertTokenizer \n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)\n",
    "\n",
    "train_dataset = Cinnamon_Dataset('/media/D/ADL2020-SPRING/project/cinnamon/train/', tokenizer)\n",
    "valid_dataset = Cinnamon_Dataset('/media/D/ADL2020-SPRING/project/cinnamon/dev/', tokenizer)\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset,\n",
    "                             batch_size=8,\n",
    "                             collate_fn=train_dataset.collate_fn)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.22) or chardet (2.1.1) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from transformers import BertModel\n",
    "\n",
    "pretrained_weights = 'bert-base-chinese'\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        \n",
    "        self.bert_embedd = BertModel.from_pretrained(pretrained_weights)\n",
    "        for param in self.bert_embedd.parameters():\n",
    "            #param.requires_grad = False\n",
    "            continue \n",
    "\n",
    "        #self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        hidden_dim = 768\n",
    "        self.fc = nn.Linear(hidden_dim, 20)\n",
    "\n",
    "        self.step_loss = dict()\n",
    "    \n",
    "    def forward(self, input_ids):\n",
    "        last_hidden_states, cls_hidden = self.bert_embedd(input_ids)\n",
    "        output = self.fc(last_hidden_states)\n",
    "        return output\n",
    "   \n",
    "    def save(self, epoch, loss, path='ckpt/'):\n",
    "        self.step_loss[epoch] = loss \n",
    "        with open(f'{path}/step_loss.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.step_loss, f, indent=4)\n",
    "        torch.save({'epoch': epoch, 'loss': loss, 'state_dict': self.state_dict()},\n",
    "                  f'{path}/epoch_{epoch}.pt')\n",
    "        print(f'\\t[Info] save weight, {path}/epoch_{epoch}.pt')\n",
    "\n",
    "    def load(self, load_file):\n",
    "        if os.path.isfile(load_file):\n",
    "            self.load_state_dict(torch.load(load_file)['state_dict'])\n",
    "            print(f'\\t[Info] load weight, {load_file}')\n",
    "        else:\n",
    "            print(f'\\t[ERROR] {load_file} not exist !')\n",
    "        return self\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "def _run_train(args, model, criterion, optimizer, dataloader):\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    total_loss, acc, f1 = 0, None, None  \n",
    "    for idx, (_input, _label) in enumerate(dataloader):\n",
    "        b = _input.shape[0]\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "       \n",
    "        _predict = model(_input.cuda())\n",
    "        \n",
    "        loss = criterion(_predict, _label.type_as(_predict).cuda())\n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()*b\n",
    "        #acc, f1 = metrics(_label, _predict)\n",
    "        print(\"\\t[{}/{}] train loss:{:.3f} \".format(\n",
    "                                            idx+1,\n",
    "                                            len(dataloader),\n",
    "                                            total_loss/(idx+1)/b),\n",
    "                                    end='   \\r')\n",
    "\n",
    "    return total_loss/len(dataloader.dataset) \n",
    "    \n",
    "def _run_eval(args, model, criterion, dataloader):\n",
    " \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        total_loss, acc, f1 = 0, None, None \n",
    "        for idx, (_input, _label) in enumerate(dataloader):\n",
    "            b = _input.shape[0]\n",
    "\n",
    "            _predict = model(_input.cuda())\n",
    "\n",
    "            loss = criterion(_predict, _label.type_as(_predict).cuda())\n",
    "        \n",
    "            total_loss += loss.item()*b\n",
    "            #acc, f1 = metrics(_label, _predict)\n",
    "            print(\"\\t[{}/{}] valid loss:{:.3f} \".format(\n",
    "                                            idx+1,\n",
    "                                            len(dataloader),\n",
    "                                            total_loss/(idx+1)/b),\n",
    "                                    end='   \\r')     \n",
    "\n",
    "    return total_loss/len(dataloader.dataset) \n",
    "\n",
    "def train(args, train_dataloader, valid_dataloader):\n",
    "    torch.manual_seed(987)\n",
    "    torch.cuda.manual_seed(987)\n",
    "    \n",
    "    model = Model()\n",
    "    model.load(args.load_model).cuda() \n",
    "    \n",
    "    criterion = nn.BCEWithLogitsLoss().cuda()#pos_weight=[]).cuda()\n",
    "     \n",
    "    optimizer = torch.optim.AdamW(list(model.parameters()), \n",
    "                                  lr=args.lr,\n",
    "                                  eps=1e-8 )\n",
    "\n",
    "    for epoch in range(1,args.epoch+1):\n",
    "        print(f' Epoch {epoch}')\n",
    "            \n",
    "        loss = _run_train(args, model, criterion, optimizer, train_dataloader)\n",
    "        print(\"\\t[Info] Train avg loss:{:.4f} \".format(loss))\n",
    "        \n",
    "        loss  = _run_eval(args, model, criterion, valid_dataloader)\n",
    "        print(\"\\t[Info] Valid avg loss:{:.4f} \".format(loss))\n",
    "        \n",
    "        ## Save ckpt\n",
    "        model.save(epoch, loss, args.save_path)\n",
    "      \n",
    "        ## Update learning rate\n",
    "        if epoch>2:\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] /= 3\n",
    "                if param_group['lr'] < 1e-6:\n",
    "                    param_group['lr'] = 1e-6 \n",
    "\n",
    "        print('\\t--------------------------------------------------------')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, json, glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class ABC_dataset(Dataset):\n",
    "    def __init__(self):\n",
    "        self.data = []\n",
    "\n",
    "        print(f'\\t[Info] Load ABC_Dataset complete !! len:{self.__len__()}')              \n",
    "    def __len__(self):\n",
    "        return len(self.data) \n",
    "    def __getitem__(self, idx):        \n",
    "        context  = self.data['context'][idx]\n",
    "    \n",
    "def collate_fn(samples):\n",
    "\n",
    "    return torch.tensor([])\n",
    "\n",
    "########################################################\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from transformers import BertTokenizer \n",
    "pretrained_weights = 'cl-tohoku/bert-base-japanese-whole-word-masking'\n",
    "\n",
    "########################################################\n",
    "##################  Cinnamon Dataset  ##################\n",
    "class Cinnamon_Dataset(Dataset):\n",
    "    def __init__(self, cinnamon_path, tokenizer):\n",
    "        def get_tags(cinnamon_path):\n",
    "            tags = set()\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "                label_str = filter(lambda i:(type(i) is str), dataframe['Tag'])\n",
    "                def split(strings):\n",
    "                    out = list()\n",
    "                    for string in strings: \n",
    "                        out += string.split(\";\")\n",
    "                    return out\n",
    "                items = split(label_str)\n",
    "                tags.update(items)\n",
    "            return tuple(sorted(list(tags)))\n",
    "            #return tuple([\"[PAD]\", \"[None]\"] + sorted(list(tags)))\n",
    "        \n",
    "        def get_samples(cinnamon_path):\n",
    "            groups = []\n",
    "            files = glob.glob(f'{cinnamon_path}/ca_data/*')\n",
    "            for file in files:\n",
    "                dataframe = pd.read_excel(file, encoding=\"utf8\")\n",
    "\n",
    "                p_index = dataframe.groupby('Parent Index')\n",
    "                for g in list(p_index.groups.keys()):\n",
    "                    groups.append(p_index.get_group(g))\n",
    "            return groups\n",
    "        \n",
    "        self.tokenizer = tokenizer\n",
    "        self.samples = get_samples(cinnamon_path)\n",
    "        self.tags = get_tags(cinnamon_path)\n",
    "\n",
    "        print(f'\\t[Info] Load Cannon_Dataset complete !! len:{self.__len__()}')    \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.samples) \n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.samples[idx]\n",
    "            \n",
    "    def collate_fn(self, samples):        \n",
    "        tokenizer, tags = self.tokenizer, self.tags\n",
    "            \n",
    "        CLS, SEP, PAD = tokenizer.cls_token_id, tokenizer.sep_token_id, tokenizer.pad_token_id\n",
    "        \n",
    "        def zero_vec(): \n",
    "            return [0]*len(tags)\n",
    "        \n",
    "        ## text tokenized, label vectoized\n",
    "        b_token_ids, b_output = [], []\n",
    "        for sample in samples:\n",
    "            token_ids = [CLS]\n",
    "            output = [zero_vec()]\n",
    "            for text, tag in zip(sample['Text'],sample['Tag']):\n",
    "                ids = tokenizer.encode(text)[1:-1] + [SEP]\n",
    "                label = zero_vec()\n",
    "                if isinstance(tag, str): \n",
    "                    for t in tag.split(';'):\n",
    "                        label[tags.index(t)] = 1\n",
    "                token_ids += ids\n",
    "                output += [label]*(len(ids)-1) + [zero_vec()]\n",
    "            b_token_ids.append(token_ids)\n",
    "            b_output.append(output)\n",
    "\n",
    "        ## pad to same lenght\n",
    "        max_len = min([max([len(s) for s in b_token_ids]), 512])\n",
    "        for idx,(token_ids, output) in enumerate(zip(b_token_ids, b_output)):            \n",
    "            token_ids = token_ids[:max_len]\n",
    "            token_ids += [PAD]*(max_len-len(token_ids))\n",
    "            b_token_ids[idx] = token_ids\n",
    "            \n",
    "            output = output[:max_len]\n",
    "            output += [zero_vec()]*(max_len-len(output))\n",
    "            b_output[idx] = output\n",
    "\n",
    "        return torch.tensor(b_token_ids), torch.tensor(b_output)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Info] Load Cannon_Dataset complete !! len:2011\n",
      "\t[Info] Load Cannon_Dataset complete !! len:557\n",
      "\t[ERROR] ckpt/epoch_6_model_loss_0.4579.pt not exist !\n",
      " Epoch 1\n",
      "\t[Info] Train avg loss:0.0550  \n",
      "\t[Info] Valid avg loss:0.0181 \n",
      "\t[Info] save weight, ckpt//epoch_1.pt\n",
      "\t--------------------------------------------------------\n",
      " Epoch 2\n",
      "\t[58/671] train loss:0.036    \r"
     ]
    }
   ],
   "source": [
    "#from train import train\n",
    "\n",
    "import os, warnings, argparse\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#from dataset import *#Cinnamon_Dataset, DataLoader, pretrained_weights\n",
    "\n",
    "\n",
    "def parse_args(string=None):\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--lr', default=2e-5,\n",
    "                        type=float, help='leanring rate')\n",
    "    parser.add_argument('--epoch', default=5,\n",
    "                        type=int, help='epochs')\n",
    "    parser.add_argument('--batch-size', default=3,\n",
    "                        type=int, help='batch size')\n",
    "    parser.add_argument('--gpu', default=\"0\",\n",
    "                        type=str, help=\"0:1080ti 1:1070\")\n",
    "    parser.add_argument('--num-workers', default=2,\n",
    "                        type=int, help='dataloader num workers')\n",
    "    parser.add_argument('--cinnamon-data-path', default='/media/D/ADL2020-SPRING/project/cinnamon/',\n",
    "                        type=str, help='cinnamon dataset')\n",
    "    parser.add_argument('--load-model', default='ckpt/epoch_6_model_loss_0.4579.pt',\n",
    "                        type=str, help='.pt model file ')\n",
    "    parser.add_argument('--save-path', default='ckpt/',\n",
    "                        type=str, help='.pt model file save dir')\n",
    "    \n",
    "    args = parser.parse_args() if string is None else parser.parse_args(string)\n",
    "    if not os.path.exists(args.save_path): os.makedirs(args.save_path)\n",
    "    return args\n",
    "\n",
    "#os.environ['CUDA_VISIBLE_DEVICES'] = \"0\" \n",
    "if __name__=='__main__':\n",
    "    args = parse_args('')\n",
    "    \n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = args.gpu \n",
    "    os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n",
    "    \n",
    "    ## load tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained(pretrained_weights, do_lower_case=True)\n",
    "\n",
    "    ## load dataset\n",
    "    train_dataset = Cinnamon_Dataset(f'{args.cinnamon_data_path}/train/', tokenizer)\n",
    "    valid_dataset = Cinnamon_Dataset(f'{args.cinnamon_data_path}/dev/', tokenizer)\n",
    "\n",
    "    train_dataloader = DataLoader(train_dataset,\n",
    "                                 batch_size = args.batch_size,\n",
    "                                 num_workers = args.num_workers,\n",
    "                                 collate_fn = train_dataset.collate_fn,\n",
    "                                 shuffle = True)\n",
    "    valid_dataloader = DataLoader(valid_dataset,\n",
    "                                 batch_size = args.batch_size*4,\n",
    "                                 num_workers = args.num_workers,\n",
    "                                 collate_fn = train_dataset.collate_fn)\n",
    "    \n",
    "    ## train\n",
    "    train(args, train_dataloader, valid_dataloader)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
